{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_large_dataset(file_path, chunk_size=1000000, downsample_method='uniform', n_points=10000):\n",
    "    \"\"\"\n",
    "    Plot Reading vs Seconds from a large dataset by processing it in chunks and downsampling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the data file\n",
    "    chunk_size : int\n",
    "        Number of rows to read in each chunk\n",
    "    downsample_method : str\n",
    "        Method for downsampling: 'uniform', 'mean', or 'bin'\n",
    "    n_points : int\n",
    "        Target number of points to plot\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process dataset: {Path(file_path).name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First, determine the header structure and column names\n",
    "    print(\"Examining file structure...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read first few lines to analyze header\n",
    "        header_lines = [f.readline() for _ in range(20)]  # Read up to 20 lines to find header\n",
    "    \n",
    "    # Look for the line that seems to contain column names\n",
    "    header_row = None\n",
    "    for i, line in enumerate(header_lines):\n",
    "        if 'Index' in line and ('Second' in line or 'Time' in line):\n",
    "            header_row = i\n",
    "            print(f\"Found header at line {header_row}: {line.strip()}\")\n",
    "            header_content = line.strip()\n",
    "            break\n",
    "    \n",
    "    if header_row is None:\n",
    "        # If we couldn't find the header, assume it's the last non-empty line before data\n",
    "        for i in reversed(range(len(header_lines))):\n",
    "            if header_lines[i].strip():\n",
    "                header_row = i\n",
    "                header_content = header_lines[i].strip()\n",
    "                print(f\"Using line {header_row} as header: {header_content}\")\n",
    "                break\n",
    "    \n",
    "    # Determine column names from the header\n",
    "    columns = [col.strip() for col in header_content.split(',')]\n",
    "    print(f\"Detected columns: {columns}\")\n",
    "    \n",
    "    # Find appropriate column names for time and reading\n",
    "    time_col = None\n",
    "    reading_col = None\n",
    "    \n",
    "    # Look for time/seconds column\n",
    "    time_candidates = ['Seconds', 'Time', 'seconds', 'time']\n",
    "    for col in columns:\n",
    "        if col in time_candidates or any(tc in col for tc in time_candidates):\n",
    "            time_col = col\n",
    "            break\n",
    "    \n",
    "    # Look for reading column\n",
    "    reading_candidates = ['Reading', 'Value', 'reading', 'value', 'Data']\n",
    "    for col in columns:\n",
    "        if col in reading_candidates or any(rc in col for rc in reading_candidates):\n",
    "            reading_col = col\n",
    "            break\n",
    "    \n",
    "    # If we couldn't find matching columns, use Index (first column) and the second column\n",
    "    if time_col is None:\n",
    "        if 'Index' in columns:\n",
    "            time_col = 'Index'\n",
    "        else:\n",
    "            time_col = columns[0]\n",
    "        print(f\"Using '{time_col}' as time column\")\n",
    "    \n",
    "    if reading_col is None:\n",
    "        # Use the second column as reading if different from time column\n",
    "        for col in columns:\n",
    "            if col != time_col:\n",
    "                reading_col = col\n",
    "                break\n",
    "        if reading_col is None and len(columns) > 1:\n",
    "            reading_col = columns[1]\n",
    "        print(f\"Using '{reading_col}' as reading column\")\n",
    "    \n",
    "    print(f\"Selected columns: Time = '{time_col}', Reading = '{reading_col}'\")\n",
    "    \n",
    "    # First, let's find the min and max of time values to determine the range\n",
    "    min_seconds = float('inf')\n",
    "    max_seconds = float('-inf')\n",
    "    \n",
    "    print(\"Scanning file for time range...\")\n",
    "    chunks_read = 0\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Read file in chunks to determine time range\n",
    "    for chunk in pd.read_csv(file_path, skiprows=header_row, chunksize=chunk_size, \n",
    "                            usecols=[time_col, reading_col]):\n",
    "        min_seconds = min(min_seconds, chunk[time_col].min())\n",
    "        max_seconds = max(max_seconds, chunk[time_col].max())\n",
    "        chunks_read += 1\n",
    "        total_rows += len(chunk)\n",
    "        print(f\"Scanned chunk {chunks_read}, total rows: {total_rows}\")\n",
    "    \n",
    "    print(f\"Time range: {min_seconds/60:.2f} to {max_seconds/60:.2f} minutes ({min_seconds:.2f} to {max_seconds:.2f} seconds)\")\n",
    "    \n",
    "    # For binning approach\n",
    "    if downsample_method == 'bin':\n",
    "        # Create bins for time ranges\n",
    "        num_bins = n_points\n",
    "        bin_edges = np.linspace(min_seconds, max_seconds, num_bins + 1)\n",
    "        bin_width = (max_seconds - min_seconds) / num_bins\n",
    "        \n",
    "        # Arrays to store results\n",
    "        bin_counts = np.zeros(num_bins)\n",
    "        bin_sums = np.zeros(num_bins)\n",
    "        \n",
    "        print(\"Processing chunks for binning...\")\n",
    "        chunks_read = 0\n",
    "        \n",
    "        # Bin centers for plotting - calculate here but use later\n",
    "        bin_centers = min_seconds + (np.arange(num_bins) + 0.5) * bin_width\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in pd.read_csv(file_path, skiprows=header_row, chunksize=chunk_size,\n",
    "                                usecols=[time_col, reading_col]):\n",
    "            # Assign each row to a bin\n",
    "            bin_indices = np.floor((chunk[time_col] - min_seconds) / bin_width).astype(int)\n",
    "            # Handle edge case\n",
    "            bin_indices = np.clip(bin_indices, 0, num_bins - 1)\n",
    "            \n",
    "            # Update bin counts and sums for this chunk\n",
    "            for i in range(num_bins):\n",
    "                mask = (bin_indices == i)\n",
    "                bin_counts[i] += mask.sum()\n",
    "                bin_sums[i] += chunk.loc[mask, reading_col].sum()\n",
    "            \n",
    "            chunks_read += 1\n",
    "            print(f\"Processed chunk {chunks_read} for binning\")\n",
    "        \n",
    "        # Calculate mean for each bin\n",
    "        bin_means = np.zeros(num_bins)\n",
    "        for i in range(num_bins):\n",
    "            if bin_counts[i] > 0:\n",
    "                bin_means[i] = bin_sums[i] / bin_counts[i]\n",
    "        \n",
    "        # Bin centers for plotting - convert to minutes\n",
    "        bin_centers_min = bin_centers / 60.0\n",
    "        \n",
    "        # Create Plotly figure\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=bin_centers_min,\n",
    "            y=bin_means,\n",
    "            mode='lines',\n",
    "            line=dict(width=2),\n",
    "            name=f'{reading_col} (Binned Average)',\n",
    "            hovertemplate=f'<b>Time:</b> %{{x:.2f}} minutes<br><b>{reading_col}:</b> %{{y:.4f}}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{reading_col} vs {time_col} - Binned Average ({n_points} bins)',\n",
    "            xaxis_title=f'Minutes',\n",
    "            yaxis_title=f'{reading_col} (Average)',\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        \n",
    "    elif downsample_method == 'uniform':\n",
    "        # Uniform sampling - take evenly spaced chunks\n",
    "        seconds_all = []\n",
    "        readings_all = []\n",
    "        \n",
    "        # Calculate how many rows to skip between samples\n",
    "        total_rows_estimate = 20000000  # From your header info\n",
    "        skip_factor = max(1, total_rows_estimate // n_points)\n",
    "        \n",
    "        print(f\"Using uniform sampling with skip factor: {skip_factor}\")\n",
    "        \n",
    "        # Read only the rows we need\n",
    "        sampled_data = pd.read_csv(file_path, skiprows=lambda x: x == 0 or (x > header_row and x % skip_factor != 0),\n",
    "                                  usecols=[time_col, reading_col])\n",
    "        \n",
    "        print(f\"Sampled {len(sampled_data)} points from dataset\")\n",
    "        \n",
    "        # Convert seconds to minutes for plotting\n",
    "        sampled_data['time_minutes'] = sampled_data[time_col] / 60.0\n",
    "        \n",
    "        # Create Plotly figure\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sampled_data['time_minutes'],\n",
    "            y=sampled_data[reading_col],\n",
    "            mode='lines',\n",
    "            line=dict(width=1),\n",
    "            name=f'{reading_col} (Uniform Sample)',\n",
    "            hovertemplate=f'<b>Time:</b> %{{x:.2f}} minutes<br><b>{reading_col}:</b> %{{y:.4f}}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{reading_col} vs {time_col} - Uniform Sampling (approx. {n_points} points)',\n",
    "            xaxis_title=f'{time_col} (minutes)',\n",
    "            yaxis_title=reading_col,\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    \n",
    "    elif downsample_method == 'mean':\n",
    "        # Determine the number of chunks to process\n",
    "        chunk_means = []\n",
    "        chunk_times = []\n",
    "        \n",
    "        print(\"Processing chunks for mean values...\")\n",
    "        chunks_read = 0\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in pd.read_csv(file_path, skiprows=header_row, chunksize=chunk_size,\n",
    "                                usecols=[time_col, reading_col]):\n",
    "            # Calculate mean for this chunk\n",
    "            mean_reading = chunk[reading_col].mean()\n",
    "            mean_time = chunk[time_col].mean()\n",
    "            \n",
    "            chunk_means.append(mean_reading)\n",
    "            chunk_times.append(mean_time)\n",
    "            \n",
    "            chunks_read += 1\n",
    "            print(f\"Processed chunk {chunks_read} for means\")\n",
    "        \n",
    "        # Convert seconds to minutes for plotting\n",
    "        chunk_times_min = [t / 60.0 for t in chunk_times]\n",
    "        \n",
    "        # Create Plotly figure\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=chunk_times_min,\n",
    "            y=chunk_means,\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6),\n",
    "            name=f'{reading_col} (Chunk Averages)',\n",
    "            hovertemplate=f'<b>Time:</b> %{{x:.2f}} minutes<br><b>{reading_col}:</b> %{{y:.4f}}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{reading_col} vs {time_col} - Chunk Averages ({chunks_read} chunks)',\n",
    "            xaxis_title=f'{time_col} (minutes)',\n",
    "            yaxis_title=f'{reading_col} (Average)',\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the plot as HTML (interactive) and optionally as PNG\n",
    "    output_file_html = f\"{reading_col}_vs_{time_col}_minutes_{downsample_method}.html\"\n",
    "    output_file_png = f\"{reading_col}_vs_{time_col}_minutes_{downsample_method}.png\"\n",
    "    \n",
    "    # Save as interactive HTML\n",
    "    fig.write_html(output_file_html)\n",
    "    print(f\"Interactive plot saved as {output_file_html}\")\n",
    "    \n",
    "    # Optionally save as PNG (requires kaleido: pip install kaleido)\n",
    "    try:\n",
    "        fig.write_image(output_file_png, width=1200, height=600, scale=2)\n",
    "        print(f\"Static plot saved as {output_file_png}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save PNG (install kaleido for PNG export): {e}\")\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "    return output_file_html, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 files total\n",
      "3_5_6Pro_4G_tiktok_stat_64sps.csv passed. Count: 1\n",
      "3_5_6pro_LTE_insta_Dyna_T1_Prefecture_INSA_64sps.csv passed. Count: 2\n",
      "3_5_6pro_3G_tiktok_stat_64sps.csv passed. Count: 3\n",
      "2_5_6pro_3G_YTshorts_stat_64sps.csv passed. Count: 4\n",
      "1_5_6pro_LTE_tiktok_Dyna_T1_Doua_Auditorium_64sps.csv passed. Count: 5\n",
      "1_5_6pro_LTE_insta_stat_64sps.csv passed. Count: 6\n",
      "3_5_4G_tiktok_stat.csv passed. Count: 7\n",
      "❌ Error with 3_5_4G_tiktok_stat.csv: 'V_BAT,I_BAT,P_BAT,V_BB,I_BB,P_BB,V_PA,I_PA,P_PA'\n",
      "1_5_6pro_LTE_tiktok_stat_64sps.csv passed. Count: 8\n",
      "1_5_6pro_3G_insta_stat_64sps.csv passed. Count: 9\n",
      "1_5_6pro_LTE_YTshorts_stat_64sps.csv passed. Count: 10\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "from utils import dataset_analyze_rasp_ff, open_file_nf1, seconds_to_duration\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if 'result_df' not in globals():\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "result_filenames = set(result_df['File name']) if not result_df.empty else set()\n",
    "\n",
    "unique_filenames = set()\n",
    "\n",
    "# Define paths\n",
    "base_dir = './data/Experiment_Data'\n",
    "reels_dir = os.path.join(base_dir, 'SIR_Experiment','Reels')\n",
    "voice_dir = os.path.join(base_dir,'SIR_Experiment' ,'Voice call')\n",
    "pubg_dir = os.path.join(base_dir, 'SIR_Experiment','pubg')\n",
    "streaming_dir = os.path.join(base_dir, 'Video straming','db')\n",
    "\n",
    "# Automatically collect all CSV files from both folders\n",
    "reels_files = [os.path.join(reels_dir, f) for f in os.listdir(reels_dir) if f.endswith('.csv')]\n",
    "voice_files = [os.path.join(voice_dir, f) for f in os.listdir(voice_dir) if f.endswith('.csv')]\n",
    "pubg_files = [os.path.join(pubg_dir, f) for f in os.listdir(pubg_dir) if f.endswith('.csv')]\n",
    "streaming_files = [os.path.join(streaming_dir, f) for f in os.listdir(streaming_dir) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "# Combine lists\n",
    "file_list = reels_files + voice_files + pubg_files + streaming_files\n",
    "\n",
    "# Process files\n",
    "files_passed = 0\n",
    "skipped = 0\n",
    "duplicates_count = 0\n",
    "problematic_files = []\n",
    "\n",
    "print(len(file_list), \"files total\")  # Print total files\n",
    "result_df = pd.DataFrame()\n",
    "for file_path in file_list:\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Skip if already processed\n",
    "    if file_name in result_filenames:\n",
    "        print(f\"{file_name} skipped because already processed\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        files_passed += 1\n",
    "        print(f\"{file_name} passed. Count: {files_passed}\")\n",
    "        result_df=dataset_analyze_rasp_ff(file_path, result_df)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {file_name}: {e}\")\n",
    "        problematic_files.append(file_name)\n",
    "\n",
    "print(f\"\\n✅ Done. {files_passed} files processed, {skipped} skipped (already in result_df).\")\n",
    "if problematic_files:\n",
    "    print(\"⚠️ Problematic files:\", problematic_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your result_df if not done already\n",
    "# result_df = pd.read_csv(\"result_df.csv\")\n",
    "\n",
    "# Add scenario_id column\n",
    "# Create scenario_id from the relevant columns\n",
    "result_df['scenario_id'] = (\n",
    "    result_df['Device'].astype(str).str.strip() + \"_\" +\n",
    "    result_df['RAN Technology'].astype(str).str.strip() + \"_\" +\n",
    "    result_df['Platform'].astype(str).str.strip() + \"_\" +\n",
    "    result_df['Condition'].astype(str).str.strip()\n",
    ")\n",
    "# Compute average energy values per scenario\n",
    "# Clean energy columns just in case\n",
    "energy_cols = ['E_RF Jm', 'E_BAT Jm', 'E_BB Jm', 'E_PA Jm']\n",
    "result_df[energy_cols] = result_df[energy_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by scenario_id\n",
    "scenario_summary_df = result_df.groupby('scenario_id')[energy_cols].mean().reset_index()\n",
    "\n",
    "# Optional: rename for clarity\n",
    "scenario_summary_df.columns = ['scenario_id', 'E_RF_Jm', 'E_BAT_Jm', 'E_BB_Jm', 'E_PA_Jm']\n",
    "\n",
    "# Save to CSV for frontend usage\n",
    "scenario_summary_df.to_csv(\"./website/server/scenario_summary_df.csv\", index=False)\n",
    "\n",
    "\n",
    "# Show preview in notebook\n",
    "scenario_summary_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
